# Adobe India Hackathon 2025 â€” Challenge 1B  
**Persona-Driven Document Intelligence â€“ Methodology Explanation**

## ðŸ“Œ Objective

The goal is to extract highly relevant content from multiple PDFs using a persona-task input and return two things:
- Top relevant **section titles** with document/page metadata.
- Detailed **refined analysis** (e.g., recipe breakdowns or instruction summaries).

The solution must:
- Run **on CPU** only
- Use a **model < 1GB**
- **Complete in â‰¤60s** for 3â€“5 PDFs
- **Not access the internet** during execution

---

## ðŸ§  Approach Overview

### Step 1: ðŸ§¾ Input Interpretation
Each collection provides:
- A `challenge1b_input.json` file containing:
  - `"persona"` (e.g., *Food Contractor*)
  - `"job_to_be_done"` (e.g., *Prepare a vegetarian dinner menu*)
- A set of PDFs containing the source material

We combine the two into a **semantic query**:
```python
query = f"As a {persona}, I want to {task}."
```

This serves as the anchor for embedding-based similarity comparison.

---

### Step 2: ðŸ“„ PDF Text Extraction

Using `PyMuPDF (fitz)`, we extract raw text from **each page** (not entire PDF), preserving page numbers. This enables fine-grained analysis and source referencing:

```python
for page in doc:
    text = page.get_text("text")
    pages.append({"page": i + 1, "text": text})
```

Each page's text is stored per document, enabling section-level analysis.

---

### Step 3: ðŸ” Semantic Embedding and Similarity Ranking

We use the compact `all-MiniLM-L6-v2` model from `sentence-transformers`:
- Size: ~90MB (well under 1GB)
- Performance: Strong semantic understanding, fast CPU execution

We encode:
- The combined persona-task **query**
- Each PDF page text
Then compute cosine similarity:

```python
score = util.pytorch_cos_sim(query_embedding, section_embedding)
```

Pages are ranked by similarity to the query. **Top 10 sections** are retained for further analysis.

---

### Step 4: ðŸ§ª Section Title & Subsection Refinement

We extract:
- **Section Title**: First 1â€“2 lines of the top section
- **Subsection Analysis**:
  - Cleaned and converted to readable paragraph
  - If recipe-like, ingredients/instructions are split heuristically:
    ```python
    if "Ingredients" in text:
        summary = "This recipe includes ... To prepare, ..."
    ```

This ensures consistency across domains: travel, cooking, software guides, etc.

---

### Step 5: ðŸ“¤ Output Assembly

We generate:
- A top-5 list of relevant section metadata (`extracted_sections`)
- A corresponding refined explanation (`subsection_analysis`)
- Timestamp and input metadata

Structured into the required JSON format:
```json
{
  "metadata": {...},
  "extracted_sections": [...],
  "subsection_analysis": [...]
}
```

---

## âœ… Challenge Constraint Breakdown

| Constraint                             | Satisfied? | How |
|----------------------------------------|------------|-----|
| **CPU only**                           | âœ…         | No GPU dependency, pure PyTorch CPU |
| **Model < 1GB**                        | âœ…         | Model is ~90MB |
| **<60s for 3â€“5 PDFs**                  | âœ…         | Empirically tested: ~25â€“40s runtime |
| **No internet during execution**       | âœ…         | Model is downloaded at build-time |
| **Structured Output**                  | âœ…         | Matches sample JSON spec |

---

## ðŸ³ Dockerization Strategy

### ðŸ”§ Dockerfile Highlights
- Uses `python:3.10-slim` base image
- Pre-installs `sentence-transformers`, `PyMuPDF`, and `scikit-learn`
- Preloads and caches model at **build time** so runtime has no downloads

### ðŸ§ª Build
```bash
docker build -t adobe-hackathon-pipeline .
```

### ðŸš€ Run
```bash
docker run --rm \
  -v $PWD/input:/app/input \
  -v $PWD/output:/app/output \
  adobe-hackathon-pipeline
```

---

## ðŸ§  Design Strengths

- **Generalizable**: Works for travel guides, recipes, how-tos, etc.
- **Efficient**: Embedding + TFIDF reranking gives focused content
- **Readable**: Paragraph-style summaries, clean JSON
- **Modular**: Easily extendable for larger datasets or streaming input

---

## ðŸ“‚ Directory Structure

```
Challenge_1b/
â”œâ”€â”€ Collection 1/
â”‚   â”œâ”€â”€ PDFs/
â”‚   â”œâ”€â”€ challenge1b_input.json
â”‚   â””â”€â”€ challenge1b_output.json (generated)
â”œâ”€â”€ main.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ approach_explanation.md
```

---

## âœ… Summary

The pipeline is light, robust, semantically intelligent, and production-ready. It satisfies every technical constraint and produces high-quality, interpretable output across document domains â€” from recipe curation to software tutorials.
